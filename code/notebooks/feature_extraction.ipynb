{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aqvbw/Jena/research/conferences-meetings/RATIO-hackathon-23/ratio-hackathon-23/dockerize\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "#set current working directory to the root \n",
    "if not os.getcwd().endswith('ratio-hackathon-23'):\n",
    "    os.chdir('../../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from pandas import DataFrame, read_xml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_read_topics(url: str, path: Path) -> DataFrame:\n",
    "    if not path.exists():\n",
    "        with path.open(\"wb\") as file:\n",
    "            file.write(get(url).content)\n",
    "    return read_xml(path).rename(columns={\"number\": \"qid\", \"title\": \"query\"}).drop(columns=[\"description\", \"narrative\"])\n",
    "\n",
    "topics_task_1 = download_read_topics(\n",
    "    \"https://touche.webis.de/clef23/touche23-data/topics-task1.xml\",\n",
    "    Path(\"topics_task_1.xml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Should teachers get tenure?'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topics_task_1[topics_task_1.qid == 1]['query'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def clean_text(doc):\n",
    "    sentences = str(doc).split('\\n')\n",
    "    sentences = [sent_tokenize(sent) for sent in sentences if sent]\n",
    "    sentences = [item for sublist in sentences for item in sublist]\n",
    "    sentences = [s for s in sentences if len(s) > 10]\n",
    "    if len(nltk.word_tokenize(\" \".join(sentences))) > 0:\n",
    "        return \" \".join(sentences)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def fetch_original_query(qid):\n",
    "    return topics_task_1[topics_task_1.qid == qid]['query'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define feature list\n",
    "feature_list = [\n",
    "\"LENGTH_OF_DOCUMENT\",\n",
    "\"LENGTH_OF_PARAGRAPHS\",\n",
    "\"LENGTH_OF_SENTENCES\",\n",
    "\"LENGTH_OF_WORDS\",\n",
    "\"STOPWORD_NUM\",\n",
    "\"SPECCHARS_NUM\",\n",
    "\"NUMERICS_NUM\",\n",
    "\"FULLUPPERCASEWORD_NUM\",\n",
    "\"UPPERCASEWORD_NUM\",\n",
    "\"UPPERCASESENTENCE_NUM\",\n",
    "\"VOCABULARY_RICHNESS\",\n",
    "\"VOCABULARY_RICHNESS_WITHOUT_STOPS\",\n",
    "\"ACADEMIC_WORDS\",\n",
    "\"PROFANITY_WORDS\",\n",
    "\"TOPIC_KEYWORDS\",\n",
    "\"ARGUMENTATIVE_WORDS\",\n",
    "\"ARGUMENT RATIO\",\n",
    "\"SUBJECTIVITY\",\n",
    "\"LINKS_NUM\",\n",
    "\"SENTIMENT\",\n",
    "\"NAMEDENTITIES_NUM\",\n",
    "\"SPELLINGMISTAKES_NUM\",\n",
    "\"READABILITY\",\n",
    "\"SENTENCE_TYPE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.util import ngrams\n",
    "import textblob\n",
    "from textblob import Word\n",
    "import enchant\n",
    "from readability import Readability\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aqvbw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# aggregate external sources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "enchant_dict = enchant.Dict(\"en_US\") # Create an English dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local sources\n",
    "academic_word_list = Path(\"./code/notebooks/sources/academic_word_list.txt\").open('r').read().split('\\n')\n",
    "academic_word_list = [word.strip() for word in academic_word_list if word.strip() != '']\n",
    "\n",
    "profanity_word_list = Path(\"./code/notebooks/sources/profanity_word_list.txt\").open('r').read().split('\\n')\n",
    "profanity_word_list = [word.strip() for word in profanity_word_list if word.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS_FROM_QUERY = {\n",
    "    \"Are gas prices too high?\": [\"gas prices\", \"cost of fuel\", \"oil prices\"],\n",
    "    \"Are social networking sites good for our society?\": [\"social networking sites\", \"social media\", \"online platforms\", \"society\"],\n",
    "    \"Are video games art?\": [\"video games\", \"art\", \"entertainment\"],\n",
    "    \"Can alternative energy effectively replace fossil fuels?\": [\"alternative energy\", \"renewable energy\", \"fossil fuels\"],\n",
    "    \"Do animals have rights?\": [\"animal rights\", \"animal welfare\", \"animal testing\"],\n",
    "    \"Do electronic voting machines improve the voting process?\": [\"electronic voting machines\", \"voting process\", \"voting technology\"],\n",
    "    \"Do humans have free will?\": [\"free will\", \"determinism\", \"philosophy\"],\n",
    "    \"Do politicians have no right to privacy?\": [\"politicians\", \"privacy\", \"ethics\", \"public life\"],\n",
    "    \"Do standardized tests improve education?\": [\"standardized tests\", \"education\", \"testing\", \"student performance\"],\n",
    "    \"Do violent video games contribute to youth violence?\": [\"violent video games\", \"youth violence\", \"media effects\"],\n",
    "    \"Does lowering the federal corporate income tax rate create jobs?\": [\"corporate income tax\", \"tax rate\", \"job creation\"],\n",
    "    \"Does poverty cause crime?\": [\"poverty\", \"crime\", \"socioeconomic factors\"],\n",
    "    \"How should nuclear waste be stored?\": [\"nuclear waste\", \"storage\", \"radioactive material\"],\n",
    "    \"Is a college education worth it?\": [\"college education\", \"higher education\", \"return on investment\"],\n",
    "    \"Is a two-state solution an acceptable solution to the Israeli-Palestinian conflict?\": [\"two-state solution\", \"Israeli-Palestinian conflict\", \"peace process\"],\n",
    "    \"Is capitalism the best form of economy?\": [\"capitalism\", \"economy\", \"economic systems\"],\n",
    "    \"Is cell phone radiation safe?\": [\"cell phone radiation\", \"health risks\", \"radio frequency\"],\n",
    "    \"Is chess a sport?\": [\"chess\", \"sports\", \"mind sports\"],\n",
    "    \"Is communism better than democracy?\": [\"communism\", \"democracy\", \"political systems\"],\n",
    "    \"Is drinking milk healthy for humans?\": [\"milk\", \"health\", \"nutrition\"],\n",
    "    \"Is feminism still needed?\": [\"feminism\", \"gender equality\", \"women's rights\"],\n",
    "    \"Is genetically modified food unsafe?\": [\"genetically modified food\", \"GMOs\", \"food safety\"],\n",
    "    \"Is golf a sport?\": [\"golf\", \"sports\", \"athleticism\"],\n",
    "    \"Is hell a real place?\": [\"hell\", \"afterlife\", \"religious beliefs\"],\n",
    "    \"Is homework beneficial?\": [\"homework\", \"education\", \"student learning\"],\n",
    "    \"Is human activity primarily responsible for global climate change?\": [\"climate change\", \"global warming\", \"human impact\"],\n",
    "    \"Is obesity a disease?\": [\"obesity\", \"disease\", \"health\"],\n",
    "    \"Is psychology a science?\": [\"psychology\", \"science\", \"behavioral science\"],\n",
    "    \"Is sexual orientation determined at birth?\": [\"sexual orientation\", \"nature vs nurture\", \"human development\"],\n",
    "    \"Is the earth flat?\": [\"flat earth\", \"conspiracy theories\", \"science\"],\n",
    "    \"Is utilitarianism morally acceptable?\": [\"utilitarianism\", \"ethics\", \"moral philosophy\"],\n",
    "    \"Is vaping with e-cigarettes safe?\": [\"e-cigarettes\", \"vaping\", \"health risks\"],\n",
    "    \"Is wind power the best alternative energy source?\": [\"wind power\", \"alternative energy\", \"renewable energy\"],\n",
    "    \"Should Scotland become independent?\": [\"Scotland\", \"independence\", \"referendum\", \"UK politics\"],\n",
    "    \"Should Turkey join the EU?\": [\"Turkey\", \"European Union\", \"EU membership\", \"international relations\"],\n",
    "    \"Should abortion be legal?\": [\"abortion\", \"reproductive rights\", \"women's health\"],\n",
    "    \"Should adults have the right to carry a concealed handgun?\": [\"concealed carry\", \"guns\", \"Second Amendment\", \"gun control\"],\n",
    "    \"Should animals be used for scientific or commercial testing?\": [\"animal testing\", \"ethical treatment of animals\", \"scientific research\"],\n",
    "    \"Should any vaccines be required for children?\": [\"vaccines\", \"childhood immunizations\", \"public health\"],\n",
    "    \"Should birth control pills be available over the counter?\": [\"birth control\", \"contraception\", \"pharmaceuticals\"],\n",
    "    \"Should blood donations be financially compensated?\": [\"blood donations\", \"compensation\", \"donor incentives\"],\n",
    "    \"Should body cameras be mandatory for police?\": [\"police\", \"body cameras\", \"law enforcement\"],\n",
    "    \"Should bottled water be banned?\": [\"bottled water\", \"environmental impact\", \"sustainability\"],\n",
    "    \"Should bullfighting be banned?\": [\"bullfighting\", \"animal cruelty\", \"cultural practices\"],\n",
    "    \"Should children beauty contests be banned?\": [\"beauty pageants\", \"children\", \"objectification\"],\n",
    "    \"Should children have mobile phones?\": [\"children\", \"mobile phones\", \"technology\"],\n",
    "    \"Should churches remain tax-exempt?\": [\"churches\", \"tax exemption\", \"religion\"],\n",
    "    \"Should corporal punishment be used in schools?\": [\"corporal punishment\", \"discipline\", \"education\"],\n",
    "    \"Should education be free?\": [\"education\", \"tuition-free\", \"public education\"],\n",
    "    \"Should election day be a national holiday?\": [\"election day\", \"voting rights\", \"national holiday\"],\n",
    "    \"Should euthanasia or physician-assisted suicide be legal?\": [\"euthanasia\", \"physician-assisted suicide\", \"end-of-life care\"],\n",
    "    \"Should everyone get a universal basic income?\": [\"universal basic income\", \"economic policy\", \"social welfare\"],\n",
    "    \"Should felons who have completed their sentence be allowed to vote?\": [\"voting rights\", \"felony disenfranchisement\", \"criminal justice\"],\n",
    "    \"Should fighting be allowed in hockey?\": [\"hockey\", \"sports\", \"athleticism\"],\n",
    "    \"Should gay marriage be legal?\": [\"gay marriage\", \"marriage equality\", \"LGBTQ+ rights\"],\n",
    "    \"Should government spending be reduced?\": [\"government spending\", \"fiscal policy\", \"budget cuts\"],\n",
    "    \"Should hate speech be penalized more?\": [\"hate speech\", \"freedom of speech\", \"discrimination\"],\n",
    "    \"Should holders of public offices resign on bad approval ratings?\": [\"public officials\", \"approval ratings\", \"political accountability\"],\n",
    "    \"Should human cloning be banned?\": [\"human cloning\", \"biotechnology\", \"bioethics\"],\n",
    "    \"Should humans attempt to contact extra-terrestrial life?\": [\"extraterrestrial life\", \"alien contact\", \"space exploration\"],\n",
    "    \"Should insider trading be allowed?\": [\"insider trading\", \"stock market\", \"securities regulation\"],\n",
    "    \"Should marijuana be a medical option?\": [\"medical marijuana\", \"cannabis\", \"drug policy\"],\n",
    "    \"Should more gun control laws be enacted?\": [\"gun control\", \"firearms\", \"public safety\"],\n",
    "    \"Should music that glorifies violence against women be banned?\": [\"music censorship\", \"violence against women\", \"media influence\"],\n",
    "    \"Should net neutrality be restored?\": [\"net neutrality\", \"internet regulations\", \"equal access\"],\n",
    "    \"Should nuclear weapons be abolished?\": [\"nuclear weapons\", \"international security\", \"disarmament\"],\n",
    "    \"Should people become vegetarian?\": [\"vegetarianism\", \"animal rights\", \"health\"],\n",
    "    \"Should performance-enhancing drugs be accepted in sports?\": [\"performance-enhancing drugs\", \"doping\", \"fair competition\"],\n",
    "    \"Should prescription drugs be advertised directly to consumers?\": [\"direct-to-consumer advertising\", \"pharmaceutical industry\", \"healthcare\"],\n",
    "    \"Should prostitution be legal?\": [\"prostitution\", \"sex work\", \"decriminalization\"],\n",
    "    \"Should prostitution be legalized?\": [\"prostitution\", \"sex work\", \"legalization\"],\n",
    "    \"Should recreational marijuana be legal?\": [\"recreational marijuana\", \"cannabis legalization\", \"drug policy\"],\n",
    "    \"Should social networks be banned?\": [\"social media\", \"internet censorship\", \"free speech\"],\n",
    "    \"Should social security be privatized?\": [\"social security\", \"retirement benefits\", \"privatization\"],\n",
    "    \"Should student loan debt be easier to discharge in bankruptcy?\": [\"student loan debt\", \"bankruptcy\", \"financial aid\"],\n",
    "    \"Should students have to wear school uniforms?\": [\"school uniforms\", \"dress code\", \"education\"],\n",
    "    \"Should suicide be a criminal offense?\": [\"suicide\", \"mental health\", \"criminalization\"],\n",
    "    \"Should teachers get tenure?\": [\"teacher tenure\", \"academic freedom\", \"education policy\"],\n",
    "    \"Should the UNO become a world government?\": [\"UNO\", \"world government\", \"international relations\"],\n",
    "    \"Should the US Electoral college be abolished?\": [\"Electoral college\", \"voting system\", \"elections\"],\n",
    "    \"Should the death penalty be allowed?\": [\"death penalty\", \"capital punishment\", \"criminal justice\"],\n",
    "    \"Should the federal minimum wage be increased?\": [\"minimum wage\", \"income inequality\", \"labor laws\"],\n",
    "    \"Should the government allow illegal immigrants to become citizens?\": [\"illegal immigration\", \"citizenship\", \"immigration reform\"],\n",
    "    \"Should the inheritance tax be raised?\": [\"inheritance tax\", \"estate tax\", \"taxation\"],\n",
    "    \"Should the penny stay in circulation?\": [\"penny\", \"currency\", \"monetary policy\"],\n",
    "    \"Should the press be subsidized?\": [\"press subsidies\", \"media industry\", \"journalism\"],\n",
    "    \"Should the voting age be lowered?\": [\"voting age\", \"youth participation\", \"democracy\"],\n",
    "    \"Should vigilantism be legal?\": [\"vigilantism\", \"citizen justice\", \"law enforcement\"],\n",
    "    \"Should we imprison fewer people?\": [\"prison reform\", \"criminal justice system\", \"mass incarceration\"],\n",
    "    \"Was the Iraq war legal?\": [\"Iraq war\", \"international law\", \"foreign policy\"],\n",
    "    \"Are gender or racial quotas effective?\": [\"gender quotas\", \"racial quotas\", \"diversity\"],\n",
    "    \"Can terrorism be justified?\": [\"terrorism\", \"justification\", \"violence\"],\n",
    "    \"Do we need cash?\": [\"cash\", \"digital currency\", \"payment systems\"],\n",
    "    \"Do we need sex education in schools?\": [\"sex education\", \"comprehensive education\", \"sexual health\"],\n",
    "    \"How can we combat cyberbullying?\": [\"cyberbullying\", \"online harassment\", \"internet safety\"],\n",
    "    \"Should agricultural subsidies be reduced?\": [\"agricultural subsidies\", \"farming industry\", \"government policy\"],\n",
    "    \"Should all museums be free of charge?\": [\"museums\", \"cultural institutions\", \"accessibility\"],\n",
    "    \"Should celibacy be abolished?\": [\"celibacy\", \"religious practice\", \"priesthood\"],\n",
    "    \"Should endangered species be protected?\": [\"endangered species\", \"conservation\", \"biodiversity\"],\n",
    "    \"Should stem cell research be expanded?\": [\"stem cell research\", \"medical research\", \"biotechnology\"]\n",
    "}\n",
    "KEYWORDS_FROM_QUERY = {k: [word.lower() for word in v] for k, v in KEYWORDS_FROM_QUERY.items()}\n",
    "\n",
    "ARGUMENTATIVE_WORDS_LIST= ['Therefore', 'In conclusion', 'Furthermore', 'Moreover', 'Consequently', 'As a result', 'However', 'Nevertheless', 'On the other hand', 'Despite this', 'Although', 'Nonetheless', 'In contrast', 'Whereas', 'Firstly', 'Secondly', 'Thirdly', 'Additionally', 'Another key point', 'Lastly', 'It is evident that', 'It is clear that', 'It is apparent that', 'It is undeniable that', 'This supports the idea that', 'This suggests that', 'One could argue that', 'It can be argued that', 'This illustrates that', 'This demonstrates that']\n",
    "ARGUMENTATIVE_WORDS_LIST = [word.lower() for word in ARGUMENTATIVE_WORDS_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def get_avg_length_of_elements(text):\n",
    "    total_length = 0\n",
    "    for item in text:\n",
    "        total_length += len(item)\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        average_length = total_length / len(text)\n",
    "        return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extractions\n",
    "def extract_length_of_document(text):\n",
    "    \"\"\"Extracts the length of the document.\"\"\"\n",
    "    doc_length = len(text)\n",
    "    return doc_length\n",
    "\n",
    "def extract_length_of_paragraphs(text):\n",
    "    \"\"\"Extracts the length of paragraphs.\"\"\"\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    paragraph_lengths = [nltk.sent_tokenize(paragraph) for paragraph in paragraphs]\n",
    "    return get_avg_length_of_elements(paragraph_lengths)\n",
    "\n",
    "def extract_length_of_sentences(text):\n",
    "    \"\"\"Extracts the length of sentences.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return get_avg_length_of_elements(sentences)\n",
    "\n",
    "def extract_length_of_words(text):\n",
    "    \"\"\"Extracts the length of words.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    av_l = get_avg_length_of_elements(words)\n",
    "    return av_l\n",
    " \n",
    "def extract_number_of_stopwords(text):\n",
    "    \"\"\"Extracts the number of stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_stopwords = sum([1 for word in words if word.lower() in stop_words])\n",
    "    return (num_stopwords / len(words))\n",
    "\n",
    "def extract_number_of_special_characters(text):\n",
    "    \"\"\"Extracts the number of special characters.\"\"\"\n",
    "    special_chars = set(string.punctuation)\n",
    "    num_special_chars = sum([1 for char in text if char in special_chars])\n",
    "    return (num_special_chars / len(text))\n",
    "\n",
    "def extract_number_of_numerics(text):\n",
    "    \"\"\"Extracts the number of numerics.\"\"\"\n",
    "    num_numerics = sum([1 for char in text if char.isdigit()])\n",
    "    return (num_numerics / len(text))\n",
    "\n",
    "def extract_number_of_uppercase_words(text):\n",
    "    \"\"\"Extracts the number of UPPERCASE WORDS and Uppercase Words.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_uppercase_words = sum([1 for word in words if word.isupper()])\n",
    "    return (num_uppercase_words / len(words))\n",
    "\n",
    "def extract_number_of_uppercase_sentence(text):\n",
    "    \"\"\"Extracts the number of UPPERCASE sentences. \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_uppercase_sentences = sum([1 for sen in sentences if sen[0].isupper()])\n",
    "    return (num_uppercase_sentences / len(sentences))\n",
    "\n",
    "def extract_number_of_academic_words(text):\n",
    "    \"\"\"Extracts the of academic quality words quality texts.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    academic_word_num = 0\n",
    "    for word in words:\n",
    "        w = Word(word)\n",
    "        neutral_form = w.lemmatize()\n",
    "        if neutral_form in academic_word_list:\n",
    "            academic_word_num += 1\n",
    "    return (academic_word_num / len(words))\n",
    "\n",
    "def extract_number_of_profanity_words(text):\n",
    "    \"\"\"Extracts the of academic profanity words quality texts.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    profanity_word_num = 0\n",
    "    for word in words:\n",
    "        w = Word(word)\n",
    "        neutral_form = w.lemmatize()\n",
    "        if neutral_form in profanity_word_list:\n",
    "            profanity_word_num += 1\n",
    "    return (profanity_word_num / len(words))\n",
    "\n",
    "def extract_vocabulary_richness(text):\n",
    "    \"\"\"Extracts the vocabulary richness with stopwords.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    l_words = []\n",
    "    for word in words:\n",
    "        w = Word(word)\n",
    "        l_words.append(w.lemmatize())\n",
    "    vocab_richness = len(set(l_words)) / len(words)\n",
    "    return vocab_richness\n",
    "\n",
    "def extract_vocabulary_richness_stopwords_removed(text):\n",
    "    \"\"\"Extracts the vocabulary richness without stopwords.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_without_stopwords = [word for word in words if word.lower() not in stop_words]\n",
    "    l_words = []\n",
    "    for word in words_without_stopwords:\n",
    "        w = Word(word)\n",
    "        l_words.append(w.lemmatize())\n",
    "    vocab_richness = len(set(l_words)) / len(words_without_stopwords)\n",
    "    return vocab_richness\n",
    "\n",
    "def extract_argument_ratio(text):\n",
    "    \"\"\"Extracts the argument ratio.\"\"\"\n",
    "    network = \"tag-essays-dependency\"\n",
    "    url = \"https://demo.webis.de/targer-api/targer-api/\"\n",
    "    headers = {\"accept\" : \"application/json\", \"Content-Type\": \"text/plain\"}\n",
    "    r = requests.post(url+network, headers=headers, data=text.encode('utf-8'))\n",
    "    number_arguments = count_arguments(r.text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return number_arguments / len(words)\n",
    "\n",
    "def extract_number_of_links(text):\n",
    "    \"\"\"Extracts the number of links.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_links = text.count(\"www.\")\n",
    "    return (num_links/len(words))\n",
    "\n",
    "def extract_subjectivity(text):\n",
    "    \"\"\"Extracts the subjectivity of text per sentence.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    subjectivity = 0\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    for sentence in sentences:\n",
    "        sentiment_scores = sia.polarity_scores(sentence)\n",
    "        subjectivity += sentiment_scores['pos'] + sentiment_scores['neg']\n",
    "    return (subjectivity / len(sentences))\n",
    "\n",
    "def extract_topic_keyword(text, keyword:list):\n",
    "    \"\"\"Extracts the presence of a topic keyword in text.\"\"\"\n",
    "    tmp_text = text.lower()\n",
    "    keyword_occurrence = sum((k in tmp_text for k in keyword)) / len(keyword)\n",
    "    return keyword_occurrence\n",
    "\n",
    "def extract_argumentative_words(text):\n",
    "    \"\"\"Extracts the presence of a topic keyword in text.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tmp_text = text.lower()\n",
    "    keyword_occurrence = sum((tmp_text.count(k) for k in ARGUMENTATIVE_WORDS_LIST)) / len(words)\n",
    "    return keyword_occurrence\n",
    "\n",
    "def extract_sentiment(text):\n",
    "    \"\"\"Extracts the sentiment of the text.\"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_sentiment = sentiment_scores['compound']\n",
    "    return compound_sentiment\n",
    "\n",
    "def extract_readability(text):\n",
    "    ##TODO edit?\n",
    "    ##The library returns an error when there is less than 100 words (or more??) in the text\n",
    "    ##In the following this is a workaround by appending the same text again. \n",
    "    ##If this does not work then use a dummy text of 'average readability (ChatGPT)'\n",
    "    \"\"\"Extracts the readability of the text using Flesch-Kincaid and SMOG indices.\"\"\"\n",
    "    dummy_text = \"The importance of exercise cannot be overstated. Regular physical activity has numerous health benefits, including improved cardiovascular function, stronger muscles and bones, and decreased risk of chronic diseases such as diabetes and obesity. Additionally, exercise is a proven mood booster, helping to reduce feelings of anxiety and depression. Incorporating exercise into your daily routine doesn't have to be difficult or time-consuming. Simply taking a brisk walk or jog around the block, or doing a few sets of bodyweight exercises like push-ups and squats, can have a significant impact on your overall health and well-being. So, whether you prefer running, cycling, swimming, or any other form of physical activity, make sure to prioritize exercise as a crucial part of your daily routine.\"\n",
    "    i=0\n",
    "    while len(nltk.word_tokenize(text))<200:\n",
    "            text = text + ' ' + text\n",
    "            i+=1\n",
    "            if(i>20):\n",
    "                text = dummy_text\n",
    "                break\n",
    "    r = Readability(text)\n",
    "    r_tups= [\n",
    "        [\"READABILITY_flesch_kincaid\",r.flesch_kincaid().score],\n",
    "        [\"READABILITY_flesch\",r.flesch().score],\n",
    "        [\"READABILITY_gunning_fog\",r.gunning_fog().score],\n",
    "        [\"READABILITY_coleman_liau\",r.coleman_liau().score],\n",
    "        [\"READABILITY_dale_chall\",r.dale_chall().score],\n",
    "        [\"READABILITY_ari\",r.ari().score],\n",
    "        [\"READABILITY_linsear_write\",r.linsear_write().score],\n",
    "        [\"READABILITY_spache\",r.spache().score]\n",
    "    ]\n",
    "    return  r_tups\n",
    "\n",
    "\n",
    "def extract_ratio_of_sentence_types(text):\n",
    "    \"\"\"Extracts the ratio of different sentence types (questions, statements, etc.) in text.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    question_count = 0\n",
    "    statement_count = 0\n",
    "    exclamation_count = 0\n",
    "    for sentence in sentences:\n",
    "        if sentence.endswith(\"?\"):\n",
    "            question_count += 1\n",
    "        elif sentence.endswith(\"!\"):\n",
    "            exclamation_count += 1\n",
    "        else:\n",
    "            statement_count += 1\n",
    "    total_sentences = len(sentences)\n",
    "    question_ratio = question_count / total_sentences\n",
    "    statement_ratio = statement_count / total_sentences\n",
    "    exclamation_ratio = exclamation_count / total_sentences\n",
    "    types = [\n",
    "        [\"SENTENCE_TYPE_question\", question_ratio],\n",
    "        [\"SENTENCE_TYPE_statement\", statement_ratio],\n",
    "        [\"SENTENCE_TYPE_exclamation\", exclamation_ratio]\n",
    "    ]\n",
    "    return types \n",
    "\n",
    "\n",
    "def extract_number_of_named_entities(text):\n",
    "    named_entities = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    chunked = ne_chunk(tagged_words)\n",
    "    for chunk in chunked:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'NE':\n",
    "            named_entities.append(chunk)\n",
    "    return len(named_entities) / len(words)\n",
    "\n",
    "\n",
    "def extract_number_of_spelling_mistakes(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_spelling_mistakes = 0\n",
    "    for word in words:\n",
    "        if not enchant_dict.check(word): # Check if the word is misspelled\n",
    "            num_spelling_mistakes += 1\n",
    "    return num_spelling_mistakes/len(words)\n",
    "\n",
    "\n",
    "### special functions for argument tagging\n",
    "\n",
    "def count_arguments(text):\n",
    "    claims = count_claims(text)\n",
    "    premises = count_premises(text)\n",
    "    return claims + premises\n",
    "\n",
    "def count_claims(text):\n",
    "    count = 0\n",
    "    for item in json.loads(text):\n",
    "        for element in item:\n",
    "            if \"C\" in element['label'] and float(element['prob']) > 0.5:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_premises(text):\n",
    "    count = 0\n",
    "    for item in json.loads(text):\n",
    "        for element in item:\n",
    "            if \"B\" in element['label'] and float(element['prob']) > 0.5:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(text, feature_name, query):\n",
    "    \"\"\"Extracts the specified feature from the text.\"\"\"\n",
    "    feature_name = feature_name.upper()\n",
    "\n",
    "    if feature_name == \"LENGTH_OF_DOCUMENT\":\n",
    "        return len(text)\n",
    "    elif feature_name == \"LENGTH_OF_PARAGRAPHS\":\n",
    "        return extract_length_of_paragraphs(text)\n",
    "    elif feature_name == \"LENGTH_OF_SENTENCES\":\n",
    "        return extract_length_of_document(text)\n",
    "    elif feature_name == \"LENGTH_OF_WORDS\":\n",
    "        return extract_length_of_words(text)\n",
    "    elif feature_name == \"STOPWORD_NUM\":\n",
    "        return extract_number_of_stopwords(text)\n",
    "    elif feature_name == \"SPECCHARS_NUM\":\n",
    "        return extract_number_of_special_characters(text)\n",
    "    elif feature_name == \"NUMERICS_NUM\":\n",
    "        return extract_number_of_numerics(text)\n",
    "    elif feature_name == \"FULLUPPERCASEWORD_NUM\":\n",
    "        return extract_number_of_uppercase_words(text)\n",
    "    elif feature_name == \"UPPERCASEWORD_NUM\":\n",
    "        return extract_number_of_uppercase_words(text)\n",
    "    elif feature_name == \"UPPERCASESENTENCE_NUM\":\n",
    "        return extract_number_of_uppercase_sentence(text)\n",
    "\n",
    "    elif feature_name == \"VOCABULARY_RICHNESS\":\n",
    "        return extract_vocabulary_richness(text)\n",
    "    elif feature_name == \"VOCABULARY_RICHNESS_WITHOUT_STOPS\":\n",
    "        return extract_vocabulary_richness_stopwords_removed(text)\n",
    "    elif feature_name == \"ACADEMIC_WORDS\":\n",
    "        return extract_number_of_academic_words(text)\n",
    "    elif feature_name == \"PROFANITY_WORDS\":\n",
    "        return extract_number_of_profanity_words(text)\n",
    "    elif feature_name == \"ARGUMENTATIVE_WORDS\":\n",
    "        return extract_argumentative_words(text)\n",
    "    elif feature_name == \"ARGUMENT RATIO\":\n",
    "        return extract_argument_ratio(text)\n",
    "    elif feature_name == \"SUBJECTIVITY\":\n",
    "        return extract_subjectivity(text)\n",
    "    elif feature_name == \"LINKS_NUM\":\n",
    "        return extract_number_of_links(text)\n",
    "    elif feature_name == \"SENTIMENT\":\n",
    "        return extract_sentiment(text)\n",
    "    elif feature_name == \"NAMEDENTITIES_NUM\":\n",
    "        return extract_number_of_named_entities(text)\n",
    "    elif feature_name == \"SPELLINGMISTAKES_NUM\":\n",
    "        return extract_number_of_spelling_mistakes(text)\n",
    "    elif feature_name == 'TOPIC_KEYWORDS':\n",
    "        return extract_topic_keyword(text, keyword=KEYWORDS_FROM_QUERY[query])\n",
    "    \n",
    "    #these functions return a list of lists!\n",
    "    elif feature_name.startswith(\"READABILITY\"):\n",
    "        return extract_readability(text)\n",
    "    elif feature_name == \"SENTENCE_TYPE\":\n",
    "        return extract_ratio_of_sentence_types(text)\n",
    "    else:\n",
    "        print(\"Error: Invalid feature name.\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def annotate_features(dfold, feature_list):\n",
    "    df = dfold.copy()\n",
    "    for feature in tqdm(feature_list):\n",
    "        print(feature)\n",
    "        if feature != \"READABILITY\" and feature != \"SENTENCE_TYPE\": \n",
    "            df[feature] = -1\n",
    "            for i, row in df.iterrows():\n",
    "                df.at[i,feature] = extract_feature(row[\"clean_text\"], feature, row['query'])\n",
    "        elif feature == \"READABILITY\":\n",
    "            df[\"READABILITY_flesch_kincaid\"] = -1\n",
    "            df[\"READABILITY_flesch\"] = -1\n",
    "            df[\"READABILITY_gunning_fog\"] = -1\n",
    "            df[\"READABILITY_coleman_liau\"] = -1\n",
    "            df[\"READABILITY_dale_chall\"] = -1\n",
    "            df[\"READABILITY_ari\"] = -1\n",
    "            df[\"READABILITY_linsear_write\"] = -1\n",
    "            df[\"READABILITY_spache\"] = -1\n",
    "            for i, row in df.iterrows():\n",
    "                read_tupel = extract_feature(row[\"clean_text\"], feature, row['query'])\n",
    "                for t in read_tupel:\n",
    "                    df.loc[i, t[0]] = t[1]\n",
    "        elif feature == \"SENTENCE_TYPE\":\n",
    "            df[\"SENTENCE_TYPE_statement\"] = -1   \n",
    "            df[\"SENTENCE_TYPE_question\"] = -1   \n",
    "            df[\"SENTENCE_TYPE_exclamation\"] = -1\n",
    "            for i, row in df.iterrows():\n",
    "                read_tupel = extract_feature(row[\"clean_text\"], feature, row['query'])\n",
    "                for t in read_tupel:\n",
    "                    df.loc[i, t[0]] = t[1]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#annotated_df = annotate_features(df, feature_list)\n",
    "# saving as tsv file\n",
    "#annotated_df.to_csv('data/qrels_args_docs_train_features.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv(\"data/train-val-splits/qrels_args_docs_val.tsv\", sep='\\t')\n",
    "\n",
    "df[\"clean_text\"] = df[\"prem\"].apply(clean_text).dropna()\n",
    "df[\"query\"] = df[\"qid\"].apply(fetch_original_query)\n",
    "df = df.dropna()\n",
    "\n",
    "annotated_df = annotate_features(df, feature_list)\n",
    "# saving as tsv file\n",
    "annotated_df.to_csv(\"data/train-val-splits/qrels_args_docs_val_features_arg_ratio.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv(\"data/train-val-splits/qrels_args_docs_train.tsv\", sep='\\t')\n",
    "\n",
    "df[\"clean_text\"] = df[\"prem\"].apply(clean_text).dropna()\n",
    "df[\"query\"] = df[\"qid\"].apply(fetch_original_query)\n",
    "df = df.dropna()\n",
    "\n",
    "annotated_df = annotate_features(df, feature_list)\n",
    "# saving as tsv file\n",
    "annotated_df.to_csv(\"data/train-val-splits/qrels_args_docs_train_features_arg_ratio.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/chatnoir_10_custom_stopw_lemmas.tsv\", sep='\\t')\n",
    "\n",
    "df[\"clean_text\"] = df[\"html_plain\"].apply(clean_text).dropna()\n",
    "df[\"query\"] = df[\"qid\"].apply(fetch_original_query)\n",
    "df = df.dropna()\n",
    "\n",
    "annotated_df = annotate_features(df, feature_list)\n",
    "# saving as tsv file\n",
    "annotated_df.to_csv(\"data/qrels_args_docs_test_features_arg_ratio.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir('data'):\n",
    "    if \"ArgumentRatio\" in file and \"_val_\" in file:\n",
    "        dfs.append(pd.read_csv(os.path.join('data',file), sep='\\t'))\n",
    "        \n",
    "df_comb = pd.concat(dfs)\n",
    "df_comb = df_comb[['doc_id', 'q_id', 'ARGUMENT RATIO']]\n",
    "len(df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_comb.columns\n",
    "#df_comb[\"hash\"] = str(df_comb.q_id)+df_comb.doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read = pd.read_csv('data/qrels_args_docs_val_features.tsv', sep=\"\\t\")\n",
    "len(df_read)\n",
    "#df_read['hash'] = str(df_read.q_id)+df_read.doc_id\n",
    "\n",
    "#list(df_comb.hash) == list(df_read.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_comb[df_comb.q_id == 94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, row in df_read.iterrows():\n",
    "#    df1 = df_comb[df_comb.q_id == row.q_id]\n",
    "#    df2 = df1[df1.doc_id == row.doc_id]\n",
    "#    if len(df2) > 1:\n",
    "#        print(df2)\n",
    "#        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df_read, df_comb, on=['q_id', 'doc_id'], how='right').drop_duplicates()\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('data/qrels_args_docs_val_features_arg_ratio.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir('data'):\n",
    "    if \"ArgumentRatio\" in file and \"_chatnoir_\" in file:\n",
    "        dfs.append(pd.read_csv(os.path.join('data',file), sep='\\t'))\n",
    "        \n",
    "df_comb = pd.concat(dfs)\n",
    "df_comb = df_comb[['docno', 'qid', 'ARGUMENT RATIO']]\n",
    "len(df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read = pd.read_csv('data/qrels_args_docs_test_features.tsv', sep=\"\\t\")\n",
    "len(df_read)\n",
    "#df_read['hash'] = str(df_read.q_id)+df_read.doc_id\n",
    "\n",
    "#list(df_comb.hash) == list(df_read.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df_read, df_comb, on=['qid', 'docno'], how='right').drop_duplicates()\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('data/qrels_args_docs_test_features_arg_ratio.tsv', sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
